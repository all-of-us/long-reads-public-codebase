{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Author:</b> ...\n",
    "\n",
    "<b>Contributors:</b> ...\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "Before you start running this notebook, make sure you are using the Hail Genomics Analysis Environment. To do so,\n",
    "<br/>\n",
    "    \n",
    "<ul>\n",
    "    <li>Click on the <b>cloud analysis environment</b> icon on the righthand side of the screen.</li>\n",
    "    <li>Inside <b>Recommended environments</b>, select <b>Hail Genomics Analysis</b> which creates a cloud environment for your analyses.</li>\n",
    "    <li>This analysis can be run with <b>high compute</b> (e.g. 96 CPUs, 624 GB of RAM, 300 workers and 300 preemptibles with 4 CPUs, 15 GB of RAM).</li>\n",
    "    <li>Click on <b>Next</b>.</li>\n",
    "</ul>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<h1>Notebook Objectives</h1>\n",
    "\n",
    "This notebook subsets the short-read v7 VDS to the ~1,027 long-read samples and the ~989 samples for which GATK-SV calls are available.\n",
    "\n",
    "<b>How to Use this Notebook...</b>\n",
    "\n",
    "<b>As a tutorial:</b>\n",
    "\n",
    "...\n",
    "\n",
    "<b>As a resource:</b>\n",
    "\n",
    "...\n",
    "\n",
    "<h2>Relevant Information:</h2>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "from pysam import VariantFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a96bde31-0204-4641-9234-8a17c1c41894\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"a96bde31-0204-4641-9234-8a17c1c41894\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.1.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"a96bde31-0204-4641-9234-8a17c1c41894\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"a96bde31-0204-4641-9234-8a17c1c41894\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a96bde31-0204-4641-9234-8a17c1c41894\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hail as hl\n",
    "from hail.plot import show\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mt_exists(gcs_path):\n",
    "    (gcs_bucket_name, gcs_obj) = re.split(\"\\/\", re.sub(\"gs://\", \"\", gcs_path), maxsplit=1)\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    gcs_bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    stats = storage.Blob(bucket=gcs_bucket, name=f'{gcs_obj}/README.txt').exists(storage_client)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vds_exists(gcs_path):\n",
    "    (gcs_bucket_name, gcs_obj) = re.split(\"\\/\", re.sub(\"gs://\", \"\", gcs_path), maxsplit=1)\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    gcs_bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    stats = storage.Blob(bucket=gcs_bucket, name=f'{gcs_obj}/reference_data/README.txt').exists(storage_client)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.environ['WORKSPACE_BUCKET']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "namespace = os.environ['WORKSPACE_NAMESPACE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"AoU_srWGS_SV_PhaseI.vcf.gz\"):\n",
    "    !gsutil -m cp gs://prod-drc-broad/aou-wgs-sv/phase1/joint-vcf/AoU_srWGS_SV_PhaseI.vcf.gz .\n",
    "    !gsutil -m cp gs://prod-drc-broad/aou-wgs-sv/phase1/joint-vcf/AoU_srWGS_SV_PhaseI.vcf.gz.tbi ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"cohort_AoUSVPhaseII.v7.LRsamples.vcf.gz\"):\n",
    "    !gsutil cp gs://fc-secure-8e5a6fd7-16ae-4796-80ed-8f0463af5ff1/yulia/cohort_AoUSVPhaseII.v7.LRsamples.vcf.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sr_sv_samples = !zgrep -m1 '^#CHROM' AoU_srWGS_SV_PhaseI.vcf.gz | cut -f10- | sed 's/\\t/\\n/g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_sv_samples = !zgrep -m1 '^#CHROM' cohort_AoUSVPhaseII.v7.LRsamples.vcf.gz | cut -f10- | sed 's/\\t/\\n/g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr_sv_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"concat_annotated.sens_09.vcf.gz\"):\n",
    "    !gsutil cp gs://fc-secure-fd873afb-038d-44ed-b113-623c141cb95f/releases/sv_integration/GRCh38/v1/concat_annotated.sens_09.vcf.gz .\n",
    "        \n",
    "if not os.path.exists(\"concat_annotated.sens_07.vcf.gz\"):        \n",
    "    !gsutil cp gs://fc-secure-fd873afb-038d-44ed-b113-623c141cb95f/releases/sv_integration/GRCh38/v1/concat_annotated.sens_07.vcf.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_sens_09_vcf = 'concat_annotated.sens_09.vcf.gz'\n",
    "sv_sens_07_vcf = 'concat_annotated.sens_07.vcf.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\t10147\t0\tC\tCCCTAACCCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACAACCCTAACCCTAACAACCCTAACAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCTAACCCTAACCCAACCCAACCCTAACCCTAACCCAACCCTAACCCTAACCCTAA\t.\t.\tTRUVARI_ID=chr1-10148-INS-330;SVTYPE=INS;SVLEN=330;GTCNT=1073,0,0,1;F_MISSING=0.999069;NS=1;AN=2;AF=1;MAF=0;AC=2;AC_Het=0;AC_Hom=2;AC_Hemi=0;HWE=1;ExcHet=1\tGT:GQ:DR:DV:SCORE:CALIBRATION_SENSITIVITY:SUPP_PBSV:SUPP_SNIFFLES:SUPP_PAV\r\n",
      "chr1\t10231\t1\tC\tCCCTAACCCTAACCCCTACCCCAACCCCAACCCCAACCCCAACCCCAACCCTTAACCCTAA\t.\t.\tTRUVARI_ID=chr1-10232-INS-60;SVTYPE=INS;SVLEN=60;GTCNT=1073,0,1,0;F_MISSING=0.999069;NS=1;AN=2;AF=0.5;MAF=0.5;AC=1;AC_Het=1;AC_Hom=0;AC_Hemi=0;HWE=1;ExcHet=1\tGT:GQ:DR:DV:SCORE:CALIBRATION_SENSITIVITY:SUPP_PBSV:SUPP_SNIFFLES:SUPP_PAV\r\n",
      "chr1\t10280\t2\tAACCCTAACCCCAACCCCAACCCCAACCCCAACCCCAACCCCAACCCTAAC\tA\t.\t.\tTRUVARI_ID=chr1-10281-DEL-50;SVTYPE=DEL;SVLEN=50;GTCNT=1074,0,0,0;F_MISSING=1;NS=0;AN=0;AF=.;MAF=.;AC=0;AC_Het=0;AC_Hom=0;AC_Hemi=0;HWE=1;ExcHet=1\tGT:GQ:DR:DV:SCORE:CALIBRATION_SENSITIVITY:SUPP_PBSV:SUPP_SNIFFLES:SUPP_PAV\r\n",
      "grep: write error: Broken pipe\r\n",
      "head: error writing 'standard output': Broken pipe\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat {sv_sens_09_vcf} | zcat | head -n 2000 | grep -v '^#' | head -n 3 | cut -f1-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 chr1 10147 ['chr1-10148-INS-330', 'INS', 330, (1073, 0, 0, 1), (0.9990689754486084,), 1, 2, (1.0,), 0.0, (2,), (0,), (2,), (0,), (1.0,), (1.0,)]\n",
      "1 chr1 10231 ['chr1-10232-INS-60', 'INS', 60, (1073, 0, 1, 0), (0.9990689754486084,), 1, 2, (0.5,), 0.5, (1,), (1,), (0,), (0,), (1.0,), (1.0,)]\n",
      "2 chr1 10280 ['chr1-10281-DEL-50', 'DEL', 50, (1074, 0, 0, 0), (1.0,), 0, 0, (None,), None, (0,), (0,), (0,), (0,), (1.0,), (1.0,)]\n",
      "3 chr1 10300 ['chr1-10301-DEL-103', 'DEL', 103, (1073, 0, 0, 1), (0.9990689754486084,), 1, 2, (1.0,), 0.0, (2,), (0,), (2,), (0,), (1.0,), (1.0,)]\n",
      "4 chr1 10306 ['chr1-10307-INS-102', 'INS', 102, (1073, 0, 0, 1), (0.9990689754486084,), 1, 2, (1.0,), 0.0, (2,), (0,), (2,), (0,), (1.0,), (1.0,)]\n",
      "5 chr1 10309 ['chr1-10310-INS-106', 'INS', 106, (1073, 0, 1, 0), (0.9990689754486084,), 1, 2, (0.5,), 0.5, (1,), (1,), (0,), (0,), (1.0,), (1.0,)]\n",
      "6 chr1 10310 ['chr1-10311-INS-91', 'INS', 91, (1073, 0, 0, 1), (0.9990689754486084,), 1, 2, (1.0,), 0.0, (2,), (0,), (2,), (0,), (1.0,), (1.0,)]\n",
      "7 chr1 10327 ['chr1-10328-INS-68', 'INS', 68, (1073, 0, 0, 1), (0.9990689754486084,), 1, 2, (1.0,), 0.0, (2,), (0,), (2,), (0,), (1.0,), (1.0,)]\n",
      "8 chr1 10328 ['chr1-10329-INS-114', 'INS', 114, (1073, 0, 0, 1), (0.9990689754486084,), 1, 2, (1.0,), 0.0, (2,), (0,), (2,), (0,), (1.0,), (1.0,)]\n",
      "9 chr1 10330 ['chr1-10331-INS-139', '6.29', 1, 1, 'INS', 139, (1072, 0, 0, 2), (0.9981380105018616,), 2, 4, (1.0,), 0.0, (4,), (0,), (4,), (0,), (1.0,), (1.0,)]\n",
      "10 chr1 10330 ['chr1-10331-INS-50', 'INS', 50, (1073, 0, 0, 1), (0.9990689754486084,), 1, 2, (1.0,), 0.0, (2,), (0,), (2,), (0,), (1.0,), (1.0,)]\n",
      "11 chr1 10330 ['chr1-10331-INS-67', 'INS', 67, (1073, 0, 1, 0), (0.9990689754486084,), 1, 2, (0.5,), 0.5, (1,), (1,), (0,), (0,), (1.0,), (1.0,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E::idx_find_and_load] Could not retrieve index file for 'concat_annotated.sens_09.vcf.gz'\n"
     ]
    }
   ],
   "source": [
    "sv_sens_09_in = VariantFile(sv_sens_09_vcf)  # auto-detect input format\n",
    "\n",
    "for i, rec in enumerate(sv_sens_09_in):\n",
    "    print(f'{i} {rec.chrom} {rec.pos} {rec.info.values()}')\n",
    "    \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List long read samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sv_samples = !zgrep -m1 '^#CHROM' concat_annotated.sens_09.vcf.gz | cut -f10- | sed 's/\\t/\\n/g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lr_sv_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zgrep -m1 '^#CHROM' concat_annotated.sens_09.vcf.gz | cut -f10- | sed 's/\\t/\\n/g' > samples_1074.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samples_1074.txt', 'r') as file:\n",
    "    sample_names = file.readlines()\n",
    "\n",
    "sample_names = [name.strip() for name in sample_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List long read samples without HPRC samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_samples_1027 = [element for element in lr_sv_samples if not (element.startswith('HG') or element.startswith('NA'))]\n",
    "len(common_samples_1027)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List long read samples with GATK-SV calls available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_samples_989 = list(set(sr_sv_samples) & set(lr_sv_samples))\n",
    "#len(common_samples_989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_samples_990 = list(set(sr_sv_samples) & set(lr_sv_samples))\n",
    "len(common_samples_990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://saturn-f75e1fa5-6fbc-4dc6-ae19-602e6c4dd082-m.us-central1-c.c.terra-7a376e4e.internal:40299\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/AoU_DRC_WGS_LongReads_PacBio/edit/hail-20250328-0502-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "spark_conf_more_ram = dict()\n",
    "spark_conf_more_ram[\"spark.executor.memory\"] = \"8g\"\n",
    "spark_conf_more_ram[\"spark.driver.memory\"] = \"196g\"\n",
    "\n",
    "# hl.init(default_reference='GRCh38', idempotent=True, spark_conf=spark_conf_more_ram)\n",
    "\n",
    "hl.init(idempotent=True, spark_conf=spark_conf_more_ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.default_reference('GRCh38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset v7 VDS to samples that have long reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 21:40:23.528 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "vds = hl.vds.read_vds('gs://prod-drc-broad/v7/wgs/with_aian_no_prod/vds/aou_srwgs_short_variants_v7_with_aian_no_prod.vds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not vds_exists(f'{bucket}/scratch/kvg/srs-subset.1027.chr22.vds'):\n",
    "    #callset_sample_filtered_1027 = hl.vds.filter_samples(vds, common_samples_1027, keep=True, remove_dead_alleles=True)\n",
    "    #callset_sample_filtered_1027.write(f'{bucket}/scratch/kvg/srs-subset.1027.chr22.vds', overwrite=True)\n",
    "    #callset_sample_filtered_1027.write(f'{bucket}/scratch/kvg/srs-subset.1027.vds', overwrite=True)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 05:04:03.617 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "callset_sample_filtered_1027 = hl.vds.read_vds(f'{bucket}/scratch/kvg/srs-subset.1027.chr22.vds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset v7 VDS to samples that have long reads and GATK-SV calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not vds_exists(f'{bucket}/scratch/kvg/srs-subset.989.vds'):\n",
    "    #callset_sample_filtered_989 = hl.vds.filter_samples(callset_sample_filtered_1027, common_samples_989, keep=True, remove_dead_alleles=True)\n",
    "    #callset_sample_filtered_989.write(f'{bucket}/scratch/kvg/srs-subset.989.vds', overwrite=True)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 21:40:59.821 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "callset_sample_filtered_989 = hl.vds.read_vds(f'{bucket}/scratch/kvg/srs-subset.989.vds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 09:06:01.630 Hail: INFO: wrote matrix table with 2437620004 rows and 990 columns in 84648 partitions to gs://fc-secure-f7d80b48-be60-426f-aa6b-f037a1bf7f34/scratch/kvg/srs-subset.990.vds/reference_data\n",
      "2025-03-28 09:08:46.425 Hail: INFO: wrote matrix table with 72130558 rows and 990 columns in 84648 partitions to gs://fc-secure-f7d80b48-be60-426f-aa6b-f037a1bf7f34/scratch/kvg/srs-subset.990.vds/variant_data\n"
     ]
    }
   ],
   "source": [
    "if not vds_exists(f'{bucket}/scratch/kvg/srs-subset.990.vds'):\n",
    "    callset_sample_filtered_990 = hl.vds.filter_samples(callset_sample_filtered_1027, common_samples_990, keep=True, remove_dead_alleles=True)\n",
    "    callset_sample_filtered_990.write(f'{bucket}/scratch/kvg/srs-subset.990.vds', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 09:08:59.493 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "callset_sample_filtered_990 = hl.vds.read_vds(f'{bucket}/scratch/kvg/srs-subset.990.vds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that we got the number of samples correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callset_sample_filtered_1027.n_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callset_sample_filtered_989.n_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callset_sample_filtered_990.n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densify subsetted VDS objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 00:28:45.412 Hail: INFO: wrote matrix table with 73161980 rows and 1027 columns in 84648 partitions to gs://fc-secure-f7d80b48-be60-426f-aa6b-f037a1bf7f34/scratch/kvg/srs-subset.1027.mt\n"
     ]
    }
   ],
   "source": [
    "if not mt_exists(f'{bucket}/scratch/kvg/srs-subset.1027.mt') or True:\n",
    "    mt_1027 = callset_sample_filtered_1027.variant_data.annotate_entries(\n",
    "        AD = hl.vds.local_to_global(callset_sample_filtered_1027.variant_data.LAD, \n",
    "                                    callset_sample_filtered_1027.variant_data.LA, \n",
    "                                    n_alleles = hl.len(callset_sample_filtered_1027.variant_data.alleles), \n",
    "                                    fill_value = 0, \n",
    "                                    number = 'R')\n",
    "    )\n",
    "    \n",
    "    mt_1027 = mt_1027.annotate_entries(GT = hl.vds.lgt_to_gt(mt_1027.LGT, mt_1027.LA))\n",
    "    mt_1027 = hl.vds.to_dense_mt(hl.vds.VariantDataset(callset_sample_filtered_1027.reference_data, mt_1027))\n",
    "    mt_1027 = mt_1027.annotate_rows(info = hl.agg.call_stats(mt_1027.GT, mt_1027.alleles))\n",
    "    mt_1027.write(f'{bucket}/scratch/kvg/srs-subset.1027.mt', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_1027 = hl.read_matrix_table(f'{bucket}/scratch/kvg/srs-subset.1027.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_qc_1027 = hl.sample_qc(mt_1027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'tranche_data': array<struct {\n",
      "        model: str, \n",
      "        truth_sensitivity: float64, \n",
      "        min_vqslod: float64, \n",
      "        filter_name: str\n",
      "    }>\n",
      "    'truth_sensitivity_snp_threshold': float64\n",
      "    'truth_sensitivity_indel_threshold': float64\n",
      "    'snp_vqslod_threshold': float64\n",
      "    'indel_vqslod_threshold': float64\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'sample_qc': struct {\n",
      "        gq_stats: struct {\n",
      "            mean: float64, \n",
      "            stdev: float64, \n",
      "            min: float64, \n",
      "            max: float64\n",
      "        }, \n",
      "        call_rate: float64, \n",
      "        n_called: int64, \n",
      "        n_not_called: int64, \n",
      "        n_filtered: int64, \n",
      "        n_hom_ref: int64, \n",
      "        n_het: int64, \n",
      "        n_hom_var: int64, \n",
      "        n_non_ref: int64, \n",
      "        n_singleton: int64, \n",
      "        n_snp: int64, \n",
      "        n_insertion: int64, \n",
      "        n_deletion: int64, \n",
      "        n_transition: int64, \n",
      "        n_transversion: int64, \n",
      "        n_star: int64, \n",
      "        r_ti_tv: float64, \n",
      "        r_het_hom_var: float64, \n",
      "        r_insertion_deletion: float64\n",
      "    }\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38>\n",
      "    'alleles': array<str>\n",
      "    'filters': set<str>\n",
      "    'as_vqsr': dict<str, struct {\n",
      "        model: str, \n",
      "        vqslod: float64, \n",
      "        yng_status: str\n",
      "    }>\n",
      "    'info': struct {\n",
      "        AC: array<int32>, \n",
      "        AF: array<float64>, \n",
      "        AN: int32, \n",
      "        homozygote_count: array<int32>\n",
      "    }\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GQ': int32\n",
      "    'GT': call\n",
      "    'LGT': call\n",
      "    'FT': bool\n",
      "    'RGQ': int32\n",
      "    'LA': array<int32>\n",
      "    'AD': array<int32>\n",
      "    'LAD': array<int32>\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mt_qc_1027.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 01:13:24.447 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n",
      "    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n",
      "[Stage 35:==================================================>(1660 + 33) / 1693]\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"22\"><div style=\"text-align: left;\"></div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"22\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">sample_qc</div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"4\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">gq_stats</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">s</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">mean</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">stdev</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">min</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">max</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">call_rate</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_called</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_not_called</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_filtered</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_hom_ref</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_het</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_hom_var</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_non_ref</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_singleton</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_snp</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_insertion</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_deletion</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_transition</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_transversion</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n_star</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">r_ti_tv</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">r_het_hom_var</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">r_insertion_deletion</div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">str</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">float64</td></tr>\n",
       "</thead><tbody><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1000151&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.85e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.38e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72716066</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">445914</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66762013</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4097921</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1856132</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5954053</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">30091</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6355492</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">792269</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">793474</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4241709</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2113783</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.21e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.98e-01</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1000513&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.79e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.32e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72731515</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">430465</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66875438</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4076768</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1779309</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5856077</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">29923</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6211279</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">777158</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">774227</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4147162</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2064117</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.29e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.00e+00</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1000920&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.85e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.38e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72718025</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">443955</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66785718</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4111733</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1820574</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5932307</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">28180</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6304855</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">791619</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">789904</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4211349</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2093506</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.26e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.00e+00</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1001399&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.88e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.36e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72714732</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">447248</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66797836</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3996124</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1920772</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5916896</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">27806</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6368180</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">800807</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">799820</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4251516</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2116664</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.08e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.00e+00</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1001980&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.83e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.34e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72746269</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">415711</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">67217496</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3725614</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1803159</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5528773</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">36753</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5946433</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">751562</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">747736</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3970485</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1975948</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.07e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.01e+00</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1002322&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.80e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.34e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72719932</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">442048</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66828626</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4026064</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1865242</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5891306</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">28609</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6305735</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">790262</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">790175</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4213357</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2092378</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.16e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.00e+00</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1002826&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.87e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.38e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72712771</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">449209</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66752399</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4121247</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1839125</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5960372</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">32002</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6342616</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">796135</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">796047</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4233637</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2108979</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.24e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.00e+00</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1004266&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.84e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.37e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72716202</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">445778</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66724227</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4108162</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1883813</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5991975</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">32626</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6407634</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">800916</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">802674</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4277358</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2130276</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.18e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.98e-01</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1005038&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.87e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.36e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72716615</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">445365</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66807892</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4123780</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1784943</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5908723</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">28700</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6257487</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">786112</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">783431</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4174980</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2082507</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.31e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.00e+00</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;1005444&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3.84e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.35e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0.00e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.90e+01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">9.94e-01</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72729426</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">432554</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">66994011</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">3874298</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1861117</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">5735415</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">49216</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">6164321</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">777891</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">775620</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4117269</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2047052</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">0</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.01e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">2.08e+00</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">1.00e+00</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+-----------+-------------------------+--------------------------+\n",
       "| s         | sample_qc.gq_stats.mean | sample_qc.gq_stats.stdev |\n",
       "+-----------+-------------------------+--------------------------+\n",
       "| str       |                 float64 |                  float64 |\n",
       "+-----------+-------------------------+--------------------------+\n",
       "| \"1000151\" |                3.85e+01 |                 1.38e+01 |\n",
       "| \"1000513\" |                3.79e+01 |                 1.32e+01 |\n",
       "| \"1000920\" |                3.85e+01 |                 1.38e+01 |\n",
       "| \"1001399\" |                3.88e+01 |                 1.36e+01 |\n",
       "| \"1001980\" |                3.83e+01 |                 1.34e+01 |\n",
       "| \"1002322\" |                3.80e+01 |                 1.34e+01 |\n",
       "| \"1002826\" |                3.87e+01 |                 1.38e+01 |\n",
       "| \"1004266\" |                3.84e+01 |                 1.37e+01 |\n",
       "| \"1005038\" |                3.87e+01 |                 1.36e+01 |\n",
       "| \"1005444\" |                3.84e+01 |                 1.35e+01 |\n",
       "+-----------+-------------------------+--------------------------+\n",
       "\n",
       "+------------------------+------------------------+---------------------+\n",
       "| sample_qc.gq_stats.min | sample_qc.gq_stats.max | sample_qc.call_rate |\n",
       "+------------------------+------------------------+---------------------+\n",
       "|                float64 |                float64 |             float64 |\n",
       "+------------------------+------------------------+---------------------+\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "|               0.00e+00 |               9.90e+01 |            9.94e-01 |\n",
       "+------------------------+------------------------+---------------------+\n",
       "\n",
       "+--------------------+------------------------+----------------------+\n",
       "| sample_qc.n_called | sample_qc.n_not_called | sample_qc.n_filtered |\n",
       "+--------------------+------------------------+----------------------+\n",
       "|              int64 |                  int64 |                int64 |\n",
       "+--------------------+------------------------+----------------------+\n",
       "|           72716066 |                      0 |               445914 |\n",
       "|           72731515 |                      0 |               430465 |\n",
       "|           72718025 |                      0 |               443955 |\n",
       "|           72714732 |                      0 |               447248 |\n",
       "|           72746269 |                      0 |               415711 |\n",
       "|           72719932 |                      0 |               442048 |\n",
       "|           72712771 |                      0 |               449209 |\n",
       "|           72716202 |                      0 |               445778 |\n",
       "|           72716615 |                      0 |               445365 |\n",
       "|           72729426 |                      0 |               432554 |\n",
       "+--------------------+------------------------+----------------------+\n",
       "\n",
       "+---------------------+-----------------+---------------------+\n",
       "| sample_qc.n_hom_ref | sample_qc.n_het | sample_qc.n_hom_var |\n",
       "+---------------------+-----------------+---------------------+\n",
       "|               int64 |           int64 |               int64 |\n",
       "+---------------------+-----------------+---------------------+\n",
       "|            66762013 |         4097921 |             1856132 |\n",
       "|            66875438 |         4076768 |             1779309 |\n",
       "|            66785718 |         4111733 |             1820574 |\n",
       "|            66797836 |         3996124 |             1920772 |\n",
       "|            67217496 |         3725614 |             1803159 |\n",
       "|            66828626 |         4026064 |             1865242 |\n",
       "|            66752399 |         4121247 |             1839125 |\n",
       "|            66724227 |         4108162 |             1883813 |\n",
       "|            66807892 |         4123780 |             1784943 |\n",
       "|            66994011 |         3874298 |             1861117 |\n",
       "+---------------------+-----------------+---------------------+\n",
       "\n",
       "+---------------------+-----------------------+-----------------+\n",
       "| sample_qc.n_non_ref | sample_qc.n_singleton | sample_qc.n_snp |\n",
       "+---------------------+-----------------------+-----------------+\n",
       "|               int64 |                 int64 |           int64 |\n",
       "+---------------------+-----------------------+-----------------+\n",
       "|             5954053 |                 30091 |         6355492 |\n",
       "|             5856077 |                 29923 |         6211279 |\n",
       "|             5932307 |                 28180 |         6304855 |\n",
       "|             5916896 |                 27806 |         6368180 |\n",
       "|             5528773 |                 36753 |         5946433 |\n",
       "|             5891306 |                 28609 |         6305735 |\n",
       "|             5960372 |                 32002 |         6342616 |\n",
       "|             5991975 |                 32626 |         6407634 |\n",
       "|             5908723 |                 28700 |         6257487 |\n",
       "|             5735415 |                 49216 |         6164321 |\n",
       "+---------------------+-----------------------+-----------------+\n",
       "\n",
       "+-----------------------+----------------------+------------------------+\n",
       "| sample_qc.n_insertion | sample_qc.n_deletion | sample_qc.n_transition |\n",
       "+-----------------------+----------------------+------------------------+\n",
       "|                 int64 |                int64 |                  int64 |\n",
       "+-----------------------+----------------------+------------------------+\n",
       "|                792269 |               793474 |                4241709 |\n",
       "|                777158 |               774227 |                4147162 |\n",
       "|                791619 |               789904 |                4211349 |\n",
       "|                800807 |               799820 |                4251516 |\n",
       "|                751562 |               747736 |                3970485 |\n",
       "|                790262 |               790175 |                4213357 |\n",
       "|                796135 |               796047 |                4233637 |\n",
       "|                800916 |               802674 |                4277358 |\n",
       "|                786112 |               783431 |                4174980 |\n",
       "|                777891 |               775620 |                4117269 |\n",
       "+-----------------------+----------------------+------------------------+\n",
       "\n",
       "+--------------------------+------------------+-------------------+\n",
       "| sample_qc.n_transversion | sample_qc.n_star | sample_qc.r_ti_tv |\n",
       "+--------------------------+------------------+-------------------+\n",
       "|                    int64 |            int64 |           float64 |\n",
       "+--------------------------+------------------+-------------------+\n",
       "|                  2113783 |                0 |          2.01e+00 |\n",
       "|                  2064117 |                0 |          2.01e+00 |\n",
       "|                  2093506 |                0 |          2.01e+00 |\n",
       "|                  2116664 |                0 |          2.01e+00 |\n",
       "|                  1975948 |                0 |          2.01e+00 |\n",
       "|                  2092378 |                0 |          2.01e+00 |\n",
       "|                  2108979 |                0 |          2.01e+00 |\n",
       "|                  2130276 |                0 |          2.01e+00 |\n",
       "|                  2082507 |                0 |          2.00e+00 |\n",
       "|                  2047052 |                0 |          2.01e+00 |\n",
       "+--------------------------+------------------+-------------------+\n",
       "\n",
       "+-------------------------+--------------------------------+\n",
       "| sample_qc.r_het_hom_var | sample_qc.r_insertion_deletion |\n",
       "+-------------------------+--------------------------------+\n",
       "|                 float64 |                        float64 |\n",
       "+-------------------------+--------------------------------+\n",
       "|                2.21e+00 |                       9.98e-01 |\n",
       "|                2.29e+00 |                       1.00e+00 |\n",
       "|                2.26e+00 |                       1.00e+00 |\n",
       "|                2.08e+00 |                       1.00e+00 |\n",
       "|                2.07e+00 |                       1.01e+00 |\n",
       "|                2.16e+00 |                       1.00e+00 |\n",
       "|                2.24e+00 |                       1.00e+00 |\n",
       "|                2.18e+00 |                       9.98e-01 |\n",
       "|                2.31e+00 |                       1.00e+00 |\n",
       "|                2.08e+00 |                       1.00e+00 |\n",
       "+-------------------------+--------------------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt_qc_1027.cols().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 23:31:19.638 Hail: INFO: wrote matrix table with 72103826 rows and 989 columns in 84648 partitions to gs://fc-secure-f7d80b48-be60-426f-aa6b-f037a1bf7f34/scratch/kvg/srs-subset.989.mt\n"
     ]
    }
   ],
   "source": [
    "if not mt_exists(f'{bucket}/scratch/kvg/srs-subset.989.mt') or True:\n",
    "    mt_989 = callset_sample_filtered_989.variant_data.annotate_entries(\n",
    "        AD = hl.vds.local_to_global(callset_sample_filtered_989.variant_data.LAD, \n",
    "                                    callset_sample_filtered_989.variant_data.LA, \n",
    "                                    n_alleles = hl.len(callset_sample_filtered_989.variant_data.alleles), \n",
    "                                    fill_value = 0, \n",
    "                                    number = 'R')\n",
    "    )\n",
    "    \n",
    "    mt_989 = mt_989.annotate_entries(GT = hl.vds.lgt_to_gt(mt_989.LGT, mt_989.LA))\n",
    "    mt_989 = hl.vds.to_dense_mt(hl.vds.VariantDataset(callset_sample_filtered_989.reference_data, mt_989))\n",
    "    mt_989 = mt_989.annotate_rows(info = hl.agg.call_stats(mt_989.GT, mt_989.alleles))\n",
    "    mt_989.write(f'{bucket}/scratch/kvg/srs-subset.989.mt', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_989 = hl.read_matrix_table(f'{bucket}/scratch/kvg/srs-subset.989.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_qc_989 = hl.sample_qc(mt_989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'tranche_data': array<struct {\n",
      "        model: str, \n",
      "        truth_sensitivity: float64, \n",
      "        min_vqslod: float64, \n",
      "        filter_name: str\n",
      "    }>\n",
      "    'truth_sensitivity_snp_threshold': float64\n",
      "    'truth_sensitivity_indel_threshold': float64\n",
      "    'snp_vqslod_threshold': float64\n",
      "    'indel_vqslod_threshold': float64\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'sample_qc': struct {\n",
      "        gq_stats: struct {\n",
      "            mean: float64, \n",
      "            stdev: float64, \n",
      "            min: float64, \n",
      "            max: float64\n",
      "        }, \n",
      "        call_rate: float64, \n",
      "        n_called: int64, \n",
      "        n_not_called: int64, \n",
      "        n_filtered: int64, \n",
      "        n_hom_ref: int64, \n",
      "        n_het: int64, \n",
      "        n_hom_var: int64, \n",
      "        n_non_ref: int64, \n",
      "        n_singleton: int64, \n",
      "        n_snp: int64, \n",
      "        n_insertion: int64, \n",
      "        n_deletion: int64, \n",
      "        n_transition: int64, \n",
      "        n_transversion: int64, \n",
      "        n_star: int64, \n",
      "        r_ti_tv: float64, \n",
      "        r_het_hom_var: float64, \n",
      "        r_insertion_deletion: float64\n",
      "    }\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38>\n",
      "    'alleles': array<str>\n",
      "    'filters': set<str>\n",
      "    'as_vqsr': dict<str, struct {\n",
      "        model: str, \n",
      "        vqslod: float64, \n",
      "        yng_status: str\n",
      "    }>\n",
      "    'info': struct {\n",
      "        AC: array<int32>, \n",
      "        AF: array<float64>, \n",
      "        AN: int32, \n",
      "        homozygote_count: array<int32>\n",
      "    }\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GQ': int32\n",
      "    'GT': call\n",
      "    'LGT': call\n",
      "    'FT': bool\n",
      "    'RGQ': int32\n",
      "    'LA': array<int32>\n",
      "    'AD': array<int32>\n",
      "    'LAD': array<int32>\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mt_qc_989.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                   (0 + 350) / 84648]\r"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 46.0 failed 4 times, most recent failure: Lost task 2.3 in stage 46.0 (TID 959727) (saturn-919ba7fe-2d8c-4e1d-945c-229767cf9700-w-276.us-central1-c.c.terra-7a376e4e.internal executor 3617): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2257)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2276)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2301)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:429)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:82)\n\tat __C18282Compiled.__m18286begin_group_0(Emit.scala)\n\tat __C18282Compiled.__m18284split_Block(Emit.scala)\n\tat __C18282Compiled.apply(Emit.scala)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:82)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:82)\n\tat is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:28)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:168)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:162)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45)\n\tat is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630)\n\tat is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\norg.apache.hadoop.ipc.RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n\n\nHail version: 0.2.130.post1-c69cd67afb8b\nError summary: RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2157\u001b[0m, in \u001b[0;36mTable._Show.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2154\u001b[0m, in \u001b[0;36mTable._Show.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ascii_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2180\u001b[0m, in \u001b[0;36mTable._Show._ascii_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m s[: truncate \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[0;32m-> 2180\u001b[0m rows, has_more, dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2181\u001b[0m fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dtype)\n\u001b[1;32m   2182\u001b[0m trunc_fields \u001b[38;5;241m=\u001b[39m [trunc(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2164\u001b[0m, in \u001b[0;36mTable._Show.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     row_dtype \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   2163\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: hl\u001b[38;5;241m.\u001b[39m_showstr(v) \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m-> 2164\u001b[0m     rows, has_more \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_n\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m (rows, has_more, row_dtype)\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2310\u001b[0m, in \u001b[0;36mTable._take_n\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     has_more \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2310\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2311\u001b[0m     has_more \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m n\n\u001b[1;32m   2312\u001b[0m     rows \u001b[38;5;241m=\u001b[39m rows[:n]\n",
      "File \u001b[0;32m<decorator-gen-1250>:2\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(self, n, _localize)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/typecheck/check.py:585\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    584\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:3027\u001b[0m, in \u001b[0;36mTable.take\u001b[0;34m(self, n, _localize)\u001b[0m\n\u001b[1;32m   2993\u001b[0m \u001b[38;5;129m@typecheck_method\u001b[39m(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, _localize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, _localize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect the first `n` rows of the table into a local list.\u001b[39;00m\n\u001b[1;32m   2996\u001b[0m \n\u001b[1;32m   2997\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;124;03m        List of row structs.\u001b[39;00m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_localize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-1244>:2\u001b[0m, in \u001b[0;36mcollect\u001b[0;34m(self, _localize, _timed)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/typecheck/check.py:585\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    584\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2814\u001b[0m, in \u001b[0;36mTable.collect\u001b[0;34m(self, _localize, _timed)\u001b[0m\n\u001b[1;32m   2812\u001b[0m e \u001b[38;5;241m=\u001b[39m construct_expr(rows_ir, hl\u001b[38;5;241m.\u001b[39mtarray(t\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _localize:\n\u001b[0;32m-> 2814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_timed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/spark_backend.py:226\u001b[0m, in \u001b[0;36mSparkBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fatal:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfatal\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/spark_backend.py:218\u001b[0m, in \u001b[0;36mSparkBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, ir: BaseIR, timed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_log_on_error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:190\u001b[0m, in \u001b[0;36mBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    188\u001b[0m     result, timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpc(ActionTag\u001b[38;5;241m.\u001b[39mEXECUTE, payload)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmaybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ir\u001b[38;5;241m.\u001b[39mtyp \u001b[38;5;241m==\u001b[39m tvoid:\n\u001b[1;32m    192\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:188\u001b[0m, in \u001b[0;36mBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    186\u001b[0m payload \u001b[38;5;241m=\u001b[39m ExecutePayload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_ir(ir), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamBufferSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m, timed)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     result, timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mActionTag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXECUTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmaybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/py4j_backend.py:221\u001b[0m, in \u001b[0;36mPy4JBackend._rpc\u001b[0;34m(self, action, payload)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    220\u001b[0m     error_json \u001b[38;5;241m=\u001b[39m orjson\u001b[38;5;241m.\u001b[39mloads(resp\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fatal_error_from_java_error_triplet(\n\u001b[1;32m    222\u001b[0m         error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort\u001b[39m\u001b[38;5;124m'\u001b[39m], error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpanded\u001b[39m\u001b[38;5;124m'\u001b[39m], error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent, resp\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX-Hail-Timings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFatalError\u001b[0m: RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 46.0 failed 4 times, most recent failure: Lost task 2.3 in stage 46.0 (TID 959727) (saturn-919ba7fe-2d8c-4e1d-945c-229767cf9700-w-276.us-central1-c.c.terra-7a376e4e.internal executor 3617): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2257)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2276)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2301)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:429)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:82)\n\tat __C18282Compiled.__m18286begin_group_0(Emit.scala)\n\tat __C18282Compiled.__m18284split_Block(Emit.scala)\n\tat __C18282Compiled.apply(Emit.scala)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:82)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:82)\n\tat is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:28)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:168)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:162)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45)\n\tat is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630)\n\tat is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\norg.apache.hadoop.ipc.RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C18399collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n\n\nHail version: 0.2.130.post1-c69cd67afb8b\nError summary: RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                  (0 + 1063) / 84648]\r"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 47.0 failed 4 times, most recent failure: Lost task 22.3 in stage 47.0 (TID 961784) (saturn-919ba7fe-2d8c-4e1d-945c-229767cf9700-w-276.us-central1-c.c.terra-7a376e4e.internal executor 3617): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2257)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2276)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2301)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:429)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:82)\n\tat __C20778Compiled.__m20782begin_group_0(Emit.scala)\n\tat __C20778Compiled.__m20780split_Block(Emit.scala)\n\tat __C20778Compiled.apply(Emit.scala)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:82)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:82)\n\tat is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:28)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:168)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:162)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45)\n\tat is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630)\n\tat is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\norg.apache.hadoop.ipc.RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n\n\nHail version: 0.2.130.post1-c69cd67afb8b\nError summary: RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2169\u001b[0m, in \u001b[0;36mTable._Show._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_html_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2260\u001b[0m, in \u001b[0;36mTable._Show._html_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhtml\u001b[39;00m\n\u001b[1;32m   2258\u001b[0m types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes\n\u001b[0;32m-> 2260\u001b[0m rows, has_more, dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2261\u001b[0m fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dtype)\n\u001b[1;32m   2263\u001b[0m default_td_style \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite-space: nowrap; \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax-width: 500px; \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverflow: hidden; \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-overflow: ellipsis; \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2265\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2164\u001b[0m, in \u001b[0;36mTable._Show.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     row_dtype \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   2163\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: hl\u001b[38;5;241m.\u001b[39m_showstr(v) \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m-> 2164\u001b[0m     rows, has_more \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_n\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m (rows, has_more, row_dtype)\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2310\u001b[0m, in \u001b[0;36mTable._take_n\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     has_more \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2310\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2311\u001b[0m     has_more \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m n\n\u001b[1;32m   2312\u001b[0m     rows \u001b[38;5;241m=\u001b[39m rows[:n]\n",
      "File \u001b[0;32m<decorator-gen-1250>:2\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(self, n, _localize)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/typecheck/check.py:585\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    584\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:3027\u001b[0m, in \u001b[0;36mTable.take\u001b[0;34m(self, n, _localize)\u001b[0m\n\u001b[1;32m   2993\u001b[0m \u001b[38;5;129m@typecheck_method\u001b[39m(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, _localize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, n, _localize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect the first `n` rows of the table into a local list.\u001b[39;00m\n\u001b[1;32m   2996\u001b[0m \n\u001b[1;32m   2997\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;124;03m        List of row structs.\u001b[39;00m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_localize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-1244>:2\u001b[0m, in \u001b[0;36mcollect\u001b[0;34m(self, _localize, _timed)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/typecheck/check.py:585\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    584\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:2814\u001b[0m, in \u001b[0;36mTable.collect\u001b[0;34m(self, _localize, _timed)\u001b[0m\n\u001b[1;32m   2812\u001b[0m e \u001b[38;5;241m=\u001b[39m construct_expr(rows_ir, hl\u001b[38;5;241m.\u001b[39mtarray(t\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _localize:\n\u001b[0;32m-> 2814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_timed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/spark_backend.py:226\u001b[0m, in \u001b[0;36mSparkBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fatal:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfatal\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/spark_backend.py:218\u001b[0m, in \u001b[0;36mSparkBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, ir: BaseIR, timed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_log_on_error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:190\u001b[0m, in \u001b[0;36mBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    188\u001b[0m     result, timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpc(ActionTag\u001b[38;5;241m.\u001b[39mEXECUTE, payload)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmaybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ir\u001b[38;5;241m.\u001b[39mtyp \u001b[38;5;241m==\u001b[39m tvoid:\n\u001b[1;32m    192\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/backend.py:188\u001b[0m, in \u001b[0;36mBackend.execute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    186\u001b[0m payload \u001b[38;5;241m=\u001b[39m ExecutePayload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_ir(ir), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamBufferSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m, timed)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     result, timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mActionTag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXECUTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmaybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/backend/py4j_backend.py:221\u001b[0m, in \u001b[0;36mPy4JBackend._rpc\u001b[0;34m(self, action, payload)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    220\u001b[0m     error_json \u001b[38;5;241m=\u001b[39m orjson\u001b[38;5;241m.\u001b[39mloads(resp\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fatal_error_from_java_error_triplet(\n\u001b[1;32m    222\u001b[0m         error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort\u001b[39m\u001b[38;5;124m'\u001b[39m], error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpanded\u001b[39m\u001b[38;5;124m'\u001b[39m], error_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent, resp\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX-Hail-Timings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFatalError\u001b[0m: RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 47.0 failed 4 times, most recent failure: Lost task 22.3 in stage 47.0 (TID 961784) (saturn-919ba7fe-2d8c-4e1d-945c-229767cf9700-w-276.us-central1-c.c.terra-7a376e4e.internal executor 3617): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2257)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2276)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2301)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:429)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:82)\n\tat __C20778Compiled.__m20782begin_group_0(Emit.scala)\n\tat __C20778Compiled.__m20780split_Block(Emit.scala)\n\tat __C20778Compiled.apply(Emit.scala)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:82)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:82)\n\tat is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:28)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21)\n\tat is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:168)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29)\n\tat is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:162)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45)\n\tat is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630)\n\tat is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\norg.apache.hadoop.ipc.RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy36.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy37.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1176)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1065)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1053)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:101)\n\tat is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:578)\n\tat is.hail.io.fs.FS.create$(FS.scala:577)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat is.hail.io.fs.FS.create(FS.scala:575)\n\tat is.hail.io.fs.FS.create$(FS.scala:575)\n\tat is.hail.io.fs.HadoopFS.create(HadoopFS.scala:85)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat __C20895collect_distributed_array_table_scan_write_prefix_sums.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87)\n\tat is.hail.utils.package$.using(package.scala:664)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86)\n\tat is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:910)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n\n\nHail version: 0.2.130.post1-c69cd67afb8b\nError summary: RemoteException: The directory item limit of /tmp/aggregate_intermediates is exceeded: limit=1048576 items=1048576\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1277)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1361)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                     (0 + 2) / 84648]\r"
     ]
    }
   ],
   "source": [
    "mt_qc_989.cols().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 09:16:05.404 Hail: INFO: wrote matrix table with 72130558 rows and 990 columns in 84648 partitions to gs://fc-secure-f7d80b48-be60-426f-aa6b-f037a1bf7f34/scratch/kvg/srs-subset.990.mt\n"
     ]
    }
   ],
   "source": [
    "if not mt_exists(f'{bucket}/scratch/kvg/srs-subset.990.mt') or True:\n",
    "    mt_990 = callset_sample_filtered_990.variant_data.annotate_entries(\n",
    "        AD = hl.vds.local_to_global(callset_sample_filtered_990.variant_data.LAD, \n",
    "                                    callset_sample_filtered_990.variant_data.LA, \n",
    "                                    n_alleles = hl.len(callset_sample_filtered_990.variant_data.alleles), \n",
    "                                    fill_value = 0, \n",
    "                                    number = 'R')\n",
    "    )\n",
    "    \n",
    "    mt_990 = mt_990.annotate_entries(GT = hl.vds.lgt_to_gt(mt_990.LGT, mt_990.LA))\n",
    "    mt_990 = hl.vds.to_dense_mt(hl.vds.VariantDataset(callset_sample_filtered_990.reference_data, mt_990))\n",
    "    mt_990 = mt_990.annotate_rows(info = hl.agg.call_stats(mt_990.GT, mt_990.alleles))\n",
    "    mt_990.write(f'{bucket}/scratch/kvg/srs-subset.990.mt', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_990 = hl.read_matrix_table(f'{bucket}/scratch/kvg/srs-subset.990.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_qc_990 = hl.sample_qc(mt_990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'tranche_data': array<struct {\n",
      "        model: str, \n",
      "        truth_sensitivity: float64, \n",
      "        min_vqslod: float64, \n",
      "        filter_name: str\n",
      "    }>\n",
      "    'truth_sensitivity_snp_threshold': float64\n",
      "    'truth_sensitivity_indel_threshold': float64\n",
      "    'snp_vqslod_threshold': float64\n",
      "    'indel_vqslod_threshold': float64\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'sample_qc': struct {\n",
      "        gq_stats: struct {\n",
      "            mean: float64, \n",
      "            stdev: float64, \n",
      "            min: float64, \n",
      "            max: float64\n",
      "        }, \n",
      "        call_rate: float64, \n",
      "        n_called: int64, \n",
      "        n_not_called: int64, \n",
      "        n_filtered: int64, \n",
      "        n_hom_ref: int64, \n",
      "        n_het: int64, \n",
      "        n_hom_var: int64, \n",
      "        n_non_ref: int64, \n",
      "        n_singleton: int64, \n",
      "        n_snp: int64, \n",
      "        n_insertion: int64, \n",
      "        n_deletion: int64, \n",
      "        n_transition: int64, \n",
      "        n_transversion: int64, \n",
      "        n_star: int64, \n",
      "        r_ti_tv: float64, \n",
      "        r_het_hom_var: float64, \n",
      "        r_insertion_deletion: float64\n",
      "    }\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38>\n",
      "    'alleles': array<str>\n",
      "    'filters': set<str>\n",
      "    'as_vqsr': dict<str, struct {\n",
      "        model: str, \n",
      "        vqslod: float64, \n",
      "        yng_status: str\n",
      "    }>\n",
      "    'info': struct {\n",
      "        AC: array<int32>, \n",
      "        AF: array<float64>, \n",
      "        AN: int32, \n",
      "        homozygote_count: array<int32>\n",
      "    }\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GT': call\n",
      "    'GQ': int32\n",
      "    'RGQ': int32\n",
      "    'LA': array<int32>\n",
      "    'FT': bool\n",
      "    'LGT': call\n",
      "    'LAD': array<int32>\n",
      "    'AD': array<int32>\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mt_qc_990.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 09:16:17.293 Hail: WARN: aggregate_cols(): Aggregates over cols ordered by 'col_key'.\n",
      "    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n",
      "[Stage 11:============================================>     (1513 + 180) / 1693]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Struct(mean=2.0096745497622273, stdev=0.002684913051356153, min=2.0012562412889787, max=2.0173091551027644, n=990, sum=1989.577804264605)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_qc_990.aggregate_cols(hl.agg.stats(mt_qc_990.sample_qc.r_ti_tv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================>           (1292 + 401) / 1693]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Struct(mean=2.183973992295268, stdev=0.10190033852555179, min=1.4475272934819965, max=2.588171391746939, n=990, sum=2162.1342523723156)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_qc_990.aggregate_cols(hl.agg.stats(mt_qc_990.sample_qc.r_het_hom_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
