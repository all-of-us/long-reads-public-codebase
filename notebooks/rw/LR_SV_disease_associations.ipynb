{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SV–Disease Association Analysis\n",
    "Aim: For SVs in high LD with SNPs from the GWAS Catalog, matched EHR data from 1,027 long-read samples were used to explore potential SV–disease associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EHR information extraction\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "CDR_version = os.getenv(\"WORKSPACE_CDR\")\n",
    "\n",
    "sample_ids_file = \"/home/jupyter/process/sample_IDs\"  # 1027 LR samples\n",
    "\n",
    "with open(sample_ids_file, \"r\") as f:\n",
    "    sample_ids = [line.strip() for line in f]  \n",
    "\n",
    "sample_ids_str = \",\".join(sample_ids)\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH combined_data AS (\n",
    "    SELECT person_id, condition_concept_id AS concept_id, 'condition' AS source\n",
    "    FROM `{CDR_version}.condition_occurrence`\n",
    "    WHERE person_id IN ({sample_ids_str})\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT person_id, observation_concept_id AS concept_id, 'observation' AS source\n",
    "    FROM `{CDR_version}.observation`\n",
    "    WHERE person_id IN ({sample_ids_str})\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT person_id, measurement_concept_id AS concept_id, 'measurement' AS source\n",
    "    FROM `{CDR_version}.measurement`\n",
    "    WHERE person_id IN ({sample_ids_str})\n",
    ")\n",
    "SELECT person_id, concept_id, source FROM combined_data\n",
    "\"\"\"\n",
    "\n",
    "df_samples = client.query(query).to_dataframe()\n",
    "\n",
    "output_file = \"/home/jupyter/process/AoU_LR_samples_EHR.csv\"\n",
    "df_samples.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240/1050027129.py:9: DtypeWarning: Columns (5,6,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_vocab = pd.read_csv(vocabulary_file, sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_file = \"/home/jupyter/process/AoU_LR_samples_EHR.csv\"\n",
    "df_samples = pd.read_csv(samples_file)\n",
    " \n",
    "df_conditions = df_samples[df_samples[\"source\"] == \"condition\"][[\"person_id\", \"concept_id\"]]\n",
    " \n",
    "vocabulary_file = \"/home/jupyter/process/vocabulary_SNOMED/CONCEPT.csv\"\n",
    "df_vocab = pd.read_csv(vocabulary_file, sep=\"\\t\")   \n",
    "\n",
    "df_vocab = df_vocab[[\"concept_id\", \"concept_name\", \"concept_class_id\", \"standard_concept\"]]\n",
    "\n",
    "df_merged = pd.merge(df_conditions, df_vocab, on=\"concept_id\", how=\"left\")\n",
    "\n",
    "output_file = \"/home/jupyter/process/LR_samples_condition_info.csv\"\n",
    "df_merged.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 \"/home/jupyter/process/LR_samples_condition_info.csv\"  > \"/home/jupyter/process/LR_samples_condition_info.unique.csv\" && sort \"/home/jupyter/process/LR_samples_condition_info.csv\" |uniq |grep -v person_id >> \"/home/jupyter/process/LR_samples_condition_info.unique.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/jupyter/process/LR_samples_condition_info.unique.csv\")\n",
    "\n",
    "disease_counts = df.groupby('concept_name')['person_id'].nunique()\n",
    "\n",
    "valid_diseases = disease_counts[(disease_counts >= 10) & (disease_counts <=838)].index\n",
    "\n",
    "filtered_df = df[df['concept_name'].isin(valid_diseases)]\n",
    "\n",
    "filtered_df.to_csv(\"/home/jupyter/process/filtered_LR_samples_condition_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -v person_id /home/jupyter/process/LR_samples_condition_info.csv |awk -F \",\" '{print $1}'  |sort |uniq > /home/jupyter/process/LR_samples_with_condition.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing genome-wide SV–disease associations with 4 jobs...\n",
      "2638740 SV–disease pairs to test\n",
      "Saved /home/jupyter/process/SV_Disease_associations.genomewide.tsv\n"
     ]
    }
   ],
   "source": [
    "for general diseases\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"/home/jupyter/process/filtered_genomewide_SVs.phase1_LD.tsv.gz\" \n",
    "OUTPUT_FILE = \"/home/jupyter/process/SV_Disease_associations.genomewide.tsv\"\n",
    "NUM_JOBS = 4\n",
    "\n",
    "# Load samples \n",
    "with open(\"/home/jupyter/process/LR_samples_with_condition.txt\") as f:\n",
    "    all_samples = {line.strip() for line in f if line.strip()}\n",
    "all_samples = set(map(str, all_samples)) \n",
    "\n",
    "# Important diseases \n",
    "with open(\"/home/jupyter/process/phase1_important_diseases.general.txt\") as f:\n",
    "    important_diseases = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "ehr_df = pd.read_csv(\"/home/jupyter/process/LR_samples_condition_info.unique.csv\", dtype=str)\n",
    "ehr_df = ehr_df[\n",
    "    (ehr_df[\"concept_class_id\"] == \"Disorder\") &\n",
    "    (ehr_df[\"standard_concept\"] == \"S\") &\n",
    "    (ehr_df[\"concept_name\"].isin(important_diseases))\n",
    "]\n",
    "\n",
    "phenotype_to_samples = (\n",
    "    ehr_df.groupby(\"concept_name\")[\"person_id\"]\n",
    "    .apply(lambda s: set(map(str, s)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "def run_fisher(sv_id, disease, meta_info, phenotype_to_samples, all_samples):\n",
    "    gene_names, sens07_id, sv_samples = meta_info[sv_id]\n",
    "    disease_samples = phenotype_to_samples[disease] & all_samples\n",
    "\n",
    "    sv_case = len(sv_samples & disease_samples)\n",
    "    sv_non_case = len(sv_samples - disease_samples)\n",
    "\n",
    "    non_sv_samples = all_samples - sv_samples\n",
    "    non_sv_case = len(non_sv_samples & disease_samples)\n",
    "    non_sv_non_case = len(non_sv_samples - disease_samples)\n",
    "\n",
    "    if min(sv_case, sv_non_case, non_sv_case, non_sv_non_case) == 0:\n",
    "        return None\n",
    "\n",
    "    table = [[sv_case, sv_non_case],\n",
    "             [non_sv_case, non_sv_non_case]]\n",
    "\n",
    "    odds_ratio, pvalue = fisher_exact(table)\n",
    "\n",
    "    try:\n",
    "        table_obj = Table2x2(table)\n",
    "        ci_lower, ci_upper = table_obj.oddsratio_confint(method=\"exact\")\n",
    "    except Exception:\n",
    "        ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "    return [\n",
    "        sv_id,\n",
    "        gene_names,\n",
    "        sens07_id,\n",
    "        len(sv_samples),\n",
    "        disease,\n",
    "        sv_case,\n",
    "        sv_non_case,\n",
    "        non_sv_case,\n",
    "        non_sv_non_case,\n",
    "        round(odds_ratio, 3),\n",
    "        round(ci_lower, 3) if not np.isnan(ci_lower) else np.nan,\n",
    "        round(ci_upper, 3) if not np.isnan(ci_upper) else np.nan,\n",
    "        pvalue,\n",
    "    ]\n",
    "\n",
    "\n",
    "def chunk_tasks(tasks, n_chunks):\n",
    "    if not tasks:\n",
    "        return []\n",
    "    chunk_size = math.ceil(len(tasks) / n_chunks)\n",
    "    return [tasks[i:i + chunk_size] for i in range(0, len(tasks), chunk_size)]\n",
    "\n",
    "\n",
    "def run_chunk(chunk, meta_info, phenotype_to_samples, all_samples):\n",
    "    results = []\n",
    "    for sv_id, disease in chunk:\n",
    "        res = run_fisher(sv_id, disease, meta_info, phenotype_to_samples, all_samples)\n",
    "        if res is not None:\n",
    "            results.append(res)\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file {INPUT_FILE} not found, exiting...\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing genome-wide SV–disease associations with {NUM_JOBS} jobs...\")\n",
    "\n",
    "    sv_df = pd.read_csv(INPUT_FILE, sep=\"\\t\", dtype=str)\n",
    "    # convert sample list string → set of sample IDs (as str)\n",
    "    sv_df[\"Sample_IDs\"] = sv_df[\"Sample_IDs\"].apply(\n",
    "        lambda x: set(map(str, x.split(\",\")))\n",
    "    )\n",
    "\n",
    "    task_list = []\n",
    "    meta_info = {}\n",
    "\n",
    "    for _, row in sv_df.iterrows():\n",
    "        sv_id = row[\"1KG_ID\"]\n",
    "        gene_names = row[\"Gene_Names\"]\n",
    "        sens07_id = row[\"sens07_ID\"]\n",
    "        sv_samples = row[\"Sample_IDs\"] & all_samples\n",
    "        control_samples = all_samples - sv_samples\n",
    "\n",
    "        if len(sv_samples) < 10 or len(control_samples) < 10:\n",
    "            continue\n",
    "\n",
    "        meta_info[sv_id] = (gene_names, sens07_id, sv_samples)\n",
    "\n",
    "        for disease in phenotype_to_samples:\n",
    "            task_list.append((sv_id, disease))\n",
    "\n",
    "    print(f\"{len(task_list)} SV–disease pairs to test\")\n",
    "\n",
    "    if not task_list:\n",
    "        print(\"No SV–disease pairs passed filters.\")\n",
    "        return\n",
    "\n",
    "    task_chunks = chunk_tasks(task_list, NUM_JOBS)\n",
    "\n",
    "    with Pool(NUM_JOBS) as pool:\n",
    "        chunked_results = pool.starmap(\n",
    "            run_chunk,\n",
    "            [\n",
    "                (chunk, meta_info, phenotype_to_samples, all_samples)\n",
    "                for chunk in task_chunks\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    results = [r for chunk in chunked_results for r in chunk]\n",
    "\n",
    "    output_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "        \"AoU_ID\", \"Gene_Names\", \"sens07_ID\", \"Sample_Count\", \"Disease\",\n",
    "        \"SV_with_Disease\", \"SV_without_Disease\", \"NonSV_with_Disease\", \"NonSV_without_Disease\",\n",
    "        \"Odds_ratio\", \"CI_lower\", \"CI_upper\", \"pvalue\"\n",
    "        ],\n",
    "    )\n",
    "    output_df.to_csv(OUTPUT_FILE, sep=\"\\t\", index=False)\n",
    "    print(f\"Saved {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing genome-wide SV–disease associations with 4 jobs...\n",
      "107982 SV–disease pairs to test\n",
      "Saved /home/jupyter/process/SV_Disease_associations.female.tsv\n"
     ]
    }
   ],
   "source": [
    "# Identification of SVs in females associated with female-specific diseases\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"/home/jupyter/process/filtered_genomewide_SVs.phase1_LD.tsv.gz\" \n",
    "OUTPUT_FILE = \"/home/jupyter/process/SV_Disease_associations.female.tsv\"\n",
    "NUM_JOBS = 4\n",
    "\n",
    "# Load female samples \n",
    "with open(\"/home/jupyter/process/AoU_LR_condition_female_samples\") as f:\n",
    "    all_samples = {line.strip() for line in f if line.strip()}\n",
    "all_samples = set(map(str, all_samples)) \n",
    "\n",
    "# Important diseases \n",
    "with open(\"/home/jupyter/process/phase1_important_diseases.female.txt\") as f:\n",
    "    important_diseases = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "ehr_df = pd.read_csv(\"/home/jupyter/process/LR_samples_condition_info.unique.csv\", dtype=str)\n",
    "ehr_df = ehr_df[\n",
    "    (ehr_df[\"concept_class_id\"] == \"Disorder\") &\n",
    "    (ehr_df[\"standard_concept\"] == \"S\") &\n",
    "    (ehr_df[\"concept_name\"].isin(important_diseases))\n",
    "]\n",
    "\n",
    "phenotype_to_samples = (\n",
    "    ehr_df.groupby(\"concept_name\")[\"person_id\"]\n",
    "    .apply(lambda s: set(map(str, s)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "def run_fisher(sv_id, disease, meta_info, phenotype_to_samples, all_samples):\n",
    "    gene_names, sens07_id, sv_samples = meta_info[sv_id]\n",
    "    disease_samples = phenotype_to_samples[disease] & all_samples\n",
    "\n",
    "    sv_case = len(sv_samples & disease_samples)\n",
    "    sv_non_case = len(sv_samples - disease_samples)\n",
    "\n",
    "    non_sv_samples = all_samples - sv_samples\n",
    "    non_sv_case = len(non_sv_samples & disease_samples)\n",
    "    non_sv_non_case = len(non_sv_samples - disease_samples)\n",
    "\n",
    "    if min(sv_case, sv_non_case, non_sv_case, non_sv_non_case) == 0:\n",
    "        return None\n",
    "\n",
    "    table = [[sv_case, sv_non_case],\n",
    "             [non_sv_case, non_sv_non_case]]\n",
    "\n",
    "    odds_ratio, pvalue = fisher_exact(table)\n",
    "\n",
    "    try:\n",
    "        table_obj = Table2x2(table)\n",
    "        ci_lower, ci_upper = table_obj.oddsratio_confint(method=\"exact\")\n",
    "    except Exception:\n",
    "        ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "    return [\n",
    "        sv_id,\n",
    "        gene_names,\n",
    "        sens07_id,\n",
    "        len(sv_samples),\n",
    "        disease,\n",
    "        sv_case,\n",
    "        sv_non_case,\n",
    "        non_sv_case,\n",
    "        non_sv_non_case,\n",
    "        round(odds_ratio, 3),\n",
    "        round(ci_lower, 3) if not np.isnan(ci_lower) else np.nan,\n",
    "        round(ci_upper, 3) if not np.isnan(ci_upper) else np.nan,\n",
    "        pvalue,\n",
    "    ]\n",
    "\n",
    "\n",
    "def chunk_tasks(tasks, n_chunks):\n",
    "    if not tasks:\n",
    "        return []\n",
    "    chunk_size = math.ceil(len(tasks) / n_chunks)\n",
    "    return [tasks[i:i + chunk_size] for i in range(0, len(tasks), chunk_size)]\n",
    "\n",
    "\n",
    "def run_chunk(chunk, meta_info, phenotype_to_samples, all_samples):\n",
    "    results = []\n",
    "    for sv_id, disease in chunk:\n",
    "        res = run_fisher(sv_id, disease, meta_info, phenotype_to_samples, all_samples)\n",
    "        if res is not None:\n",
    "            results.append(res)\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file {INPUT_FILE} not found, exiting...\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing genome-wide SV–disease associations with {NUM_JOBS} jobs...\")\n",
    "\n",
    "    sv_df = pd.read_csv(INPUT_FILE, sep=\"\\t\", dtype=str)\n",
    "    # convert sample list string → set of sample IDs (as str)\n",
    "    sv_df[\"Sample_IDs\"] = sv_df[\"Sample_IDs\"].apply(\n",
    "        lambda x: set(map(str, x.split(\",\")))\n",
    "    )\n",
    "\n",
    "    task_list = []\n",
    "    meta_info = {}\n",
    "\n",
    "    for _, row in sv_df.iterrows():\n",
    "        sv_id = row[\"1KG_ID\"]\n",
    "        gene_names = row[\"Gene_Names\"]\n",
    "        sens07_id = row[\"sens07_ID\"]\n",
    "        sv_samples = row[\"Sample_IDs\"] & all_samples\n",
    "        control_samples = all_samples - sv_samples\n",
    "\n",
    "        if len(sv_samples) < 10 or len(control_samples) < 10:\n",
    "            continue\n",
    "\n",
    "        meta_info[sv_id] = (gene_names, sens07_id, sv_samples)\n",
    "\n",
    "        for disease in phenotype_to_samples:\n",
    "            task_list.append((sv_id, disease))\n",
    "\n",
    "    print(f\"{len(task_list)} SV–disease pairs to test\")\n",
    "\n",
    "    if not task_list:\n",
    "        print(\"No SV–disease pairs passed filters.\")\n",
    "        return\n",
    "\n",
    "    task_chunks = chunk_tasks(task_list, NUM_JOBS)\n",
    "\n",
    "    with Pool(NUM_JOBS) as pool:\n",
    "        chunked_results = pool.starmap(\n",
    "            run_chunk,\n",
    "            [\n",
    "                (chunk, meta_info, phenotype_to_samples, all_samples)\n",
    "                for chunk in task_chunks\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    results = [r for chunk in chunked_results for r in chunk]\n",
    "\n",
    "    output_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "        \"AoU_ID\", \"Gene_Names\", \"sens07_ID\", \"Sample_Count\", \"Disease\",\n",
    "        \"SV_with_Disease\", \"SV_without_Disease\", \"NonSV_with_Disease\", \"NonSV_without_Disease\",\n",
    "        \"Odds_ratio\", \"CI_lower\", \"CI_upper\", \"pvalue\"\n",
    "        ],\n",
    "    )\n",
    "    output_df.to_csv(OUTPUT_FILE, sep=\"\\t\", index=False)\n",
    "    print(f\"Saved {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing genome-wide SV–disease associations with 4 jobs...\n",
      "29502 SV–disease pairs to test\n",
      "Saved /home/jupyter/process/SV_Disease_associations.male.tsv\n"
     ]
    }
   ],
   "source": [
    "#Identification of SVs in males associated with male-specific diseases\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"/home/jupyter/process/filtered_genomewide_SVs.phase1_LD.tsv.gz\" \n",
    "OUTPUT_FILE = \"/home/jupyter/process/SV_Disease_associations.male.tsv\"\n",
    "NUM_JOBS = 4\n",
    "\n",
    "# Load female samples \n",
    "with open(\"/home/jupyter/process/AoU_LR_condition_male_samples\") as f:\n",
    "    all_samples = {line.strip() for line in f if line.strip()}\n",
    "all_samples = set(map(str, all_samples)) \n",
    "\n",
    "# Important diseases \n",
    "with open(\"/home/jupyter/process/phase1_important_diseases.male.txt\") as f:\n",
    "    important_diseases = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "ehr_df = pd.read_csv(\"/home/jupyter/process/LR_samples_condition_info.unique.csv\", dtype=str)\n",
    "ehr_df = ehr_df[\n",
    "    (ehr_df[\"concept_class_id\"] == \"Disorder\") &\n",
    "    (ehr_df[\"standard_concept\"] == \"S\") &\n",
    "    (ehr_df[\"concept_name\"].isin(important_diseases))\n",
    "]\n",
    "\n",
    "phenotype_to_samples = (\n",
    "    ehr_df.groupby(\"concept_name\")[\"person_id\"]\n",
    "    .apply(lambda s: set(map(str, s)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "def run_fisher(sv_id, disease, meta_info, phenotype_to_samples, all_samples):\n",
    "    gene_names, sens07_id, sv_samples = meta_info[sv_id]\n",
    "    disease_samples = phenotype_to_samples[disease] & all_samples\n",
    "\n",
    "    sv_case = len(sv_samples & disease_samples)\n",
    "    sv_non_case = len(sv_samples - disease_samples)\n",
    "\n",
    "    non_sv_samples = all_samples - sv_samples\n",
    "    non_sv_case = len(non_sv_samples & disease_samples)\n",
    "    non_sv_non_case = len(non_sv_samples - disease_samples)\n",
    "\n",
    "    if min(sv_case, sv_non_case, non_sv_case, non_sv_non_case) == 0:\n",
    "        return None\n",
    "\n",
    "    table = [[sv_case, sv_non_case],\n",
    "             [non_sv_case, non_sv_non_case]]\n",
    "\n",
    "    odds_ratio, pvalue = fisher_exact(table)\n",
    "\n",
    "    try:\n",
    "        table_obj = Table2x2(table)\n",
    "        ci_lower, ci_upper = table_obj.oddsratio_confint(method=\"exact\")\n",
    "    except Exception:\n",
    "        ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "    return [\n",
    "        sv_id,\n",
    "        gene_names,\n",
    "        sens07_id,\n",
    "        len(sv_samples),\n",
    "        disease,\n",
    "        sv_case,\n",
    "        sv_non_case,\n",
    "        non_sv_case,\n",
    "        non_sv_non_case,\n",
    "        round(odds_ratio, 3),\n",
    "        round(ci_lower, 3) if not np.isnan(ci_lower) else np.nan,\n",
    "        round(ci_upper, 3) if not np.isnan(ci_upper) else np.nan,\n",
    "        pvalue,\n",
    "    ]\n",
    "\n",
    "\n",
    "def chunk_tasks(tasks, n_chunks):\n",
    "    if not tasks:\n",
    "        return []\n",
    "    chunk_size = math.ceil(len(tasks) / n_chunks)\n",
    "    return [tasks[i:i + chunk_size] for i in range(0, len(tasks), chunk_size)]\n",
    "\n",
    "\n",
    "def run_chunk(chunk, meta_info, phenotype_to_samples, all_samples):\n",
    "    results = []\n",
    "    for sv_id, disease in chunk:\n",
    "        res = run_fisher(sv_id, disease, meta_info, phenotype_to_samples, all_samples)\n",
    "        if res is not None:\n",
    "            results.append(res)\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file {INPUT_FILE} not found, exiting...\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing genome-wide SV–disease associations with {NUM_JOBS} jobs...\")\n",
    "\n",
    "    sv_df = pd.read_csv(INPUT_FILE, sep=\"\\t\", dtype=str)\n",
    "    # convert sample list string → set of sample IDs (as str)\n",
    "    sv_df[\"Sample_IDs\"] = sv_df[\"Sample_IDs\"].apply(\n",
    "        lambda x: set(map(str, x.split(\",\")))\n",
    "    )\n",
    "\n",
    "    task_list = []\n",
    "    meta_info = {}\n",
    "\n",
    "    for _, row in sv_df.iterrows():\n",
    "        sv_id = row[\"1KG_ID\"]\n",
    "        gene_names = row[\"Gene_Names\"]\n",
    "        sens07_id = row[\"sens07_ID\"]\n",
    "        sv_samples = row[\"Sample_IDs\"] & all_samples\n",
    "        control_samples = all_samples - sv_samples\n",
    "\n",
    "        if len(sv_samples) < 10 or len(control_samples) < 10:\n",
    "            continue\n",
    "\n",
    "        meta_info[sv_id] = (gene_names, sens07_id, sv_samples)\n",
    "\n",
    "        for disease in phenotype_to_samples:\n",
    "            task_list.append((sv_id, disease))\n",
    "\n",
    "    print(f\"{len(task_list)} SV–disease pairs to test\")\n",
    "\n",
    "    if not task_list:\n",
    "        print(\"No SV–disease pairs passed filters.\")\n",
    "        return\n",
    "\n",
    "    task_chunks = chunk_tasks(task_list, NUM_JOBS)\n",
    "\n",
    "    with Pool(NUM_JOBS) as pool:\n",
    "        chunked_results = pool.starmap(\n",
    "            run_chunk,\n",
    "            [\n",
    "                (chunk, meta_info, phenotype_to_samples, all_samples)\n",
    "                for chunk in task_chunks\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    results = [r for chunk in chunked_results for r in chunk]\n",
    "\n",
    "    output_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "        \"AoU_ID\", \"Gene_Names\", \"sens07_ID\", \"Sample_Count\", \"Disease\",\n",
    "        \"SV_with_Disease\", \"SV_without_Disease\", \"NonSV_with_Disease\", \"NonSV_without_Disease\",\n",
    "        \"Odds_ratio\", \"CI_lower\", \"CI_upper\", \"pvalue\"\n",
    "        ],\n",
    "    )\n",
    "    output_df.to_csv(OUTPUT_FILE, sep=\"\\t\", index=False)\n",
    "    print(f\"Saved {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
